# -*- coding: utf-8 -*-
"""Untitled74.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1x067WebLkU8NtXnQbVl3asNtpyJJ80
"""

# ============================================
# 1. IMPORT LIBRARIES
# ============================================

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, LSTM, Input, Concatenate
from tensorflow.keras.models import Model
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")


# ============================================
# 2. LOAD + PREPROCESS DATA
# ============================================

df = pd.read_csv("household_power_consumption.csv")
df = df.dropna()

# Select features
features = ["Global_active_power","Global_reactive_power","Voltage"]
df = df[features]

# Scale data
scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

# Create sequences
def create_dataset(data, lookback=30):
    X, y = [], []
    for i in range(len(data)-lookback):
        X.append(data[i:i+lookback])
        y.append(data[i+lookback, 0])
    return np.array(X), np.array(y)

LOOKBACK = 30
X, y = create_dataset(scaled, LOOKBACK)

# Train-test split
train_size = int(len(X)*0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]


# ============================================
# 3. CUSTOM ATTENTION LAYER
# ============================================

class Attention(Layer):
    def _init_(self):
        super(Attention, self)._init_()

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight",
                                 shape=(input_shape[-1], 1),
                                 initializer="normal")
        self.b = self.add_weight(name="att_bias",
                                 shape=(input_shape[1], 1),
                                 initializer="zeros")

    def call(self, x):
        e = tf.nn.tanh(tf.matmul(x, self.W) + self.b)
        a = tf.nn.softmax(e, axis=1)
        output = x * a
        return tf.reduce_sum(output, axis=1), a


# ============================================
# 4. BUILD LSTM + ATTENTION MODEL
# ============================================

def create_model(units=64, dropout=0.1):
    inp = Input(shape=(LOOKBACK, X.shape[2]))
    lstm_out = LSTM(units, return_sequences=True)(inp)
    att_out, att_weights = Attention()(lstm_out)
    dense = Dense(32, activation="relu")(att_out)
    out = Dense(1)(dense)

    model = Model(inputs=inp, outputs=[out, att_weights])
    model.compile(optimizer="adam", loss="mse")
    return model

model = create_model(64)
model.summary()


# ============================================
# 5. TRAIN MODEL
# ============================================

history = model.fit(
    X_train, [y_train, np.zeros((len(y_train), LOOKBACK, 1))],
    epochs=20,
    batch_size=32,
    validation_split=0.1,
    verbose=1
)


# ============================================
# 6. MODEL EVALUATION
# ============================================

pred, att = model.predict(X_test)
pred = pred.reshape(-1)

rmse = np.sqrt(mean_squared_error(y_test, pred))
mae = mean_absolute_error(y_test, pred)

print("RMSE :", rmse)
print("MAE  :", mae)


# ============================================
# 7. PLOT PREDICTIONS
# ============================================

plt.figure(figsize=(10,4))
plt.plot(y_test[:200], label="Actual")
plt.plot(pred[:200], label="Predicted")
plt.title("LSTM + Attention Forecasting")
plt.legend()
plt.show()


# ============================================
# 8. VISUALIZE ATTENTION WEIGHTS
# ============================================

plt.figure(figsize=(10,4))
plt.imshow(att[0].reshape(-1,1), cmap="viridis")
plt.colorbar()
plt.title("Attention Weights for First Test Sample")
plt.show()